<html lang="en"><head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
    <title>Lunar Lander Simulation</title>
    <meta content="" name="descriptison">
    <meta content="" name="keywords">
  
    <!-- Favicons -->
    <link href="../assets/img/favicon.png" rel="icon">
    <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">
  
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">
  
    <!-- Vendor CSS Files -->
    <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="../assets/vendor/icofont/icofont.min.css" rel="stylesheet">
    <link href="../assets/vendor/remixicon/remixicon.css" rel="stylesheet">
    <link href="../assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="../assets/vendor/venobox/venobox.css" rel="stylesheet">
  
    <!-- Template Main CSS File -->
    <link href="../assets/css/style.css" rel="stylesheet">
  
    <!-- =======================================================
    * Template Name: Personal - v2.2.0
    * Template URL: https://bootstrapmade.com/personal-free-resume-bootstrap-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
  </head>
  
  <body data-gr-c-s-loaded="true">
  
  <!-- ======= Portfolio Details ======= -->
    <main id="main">
      <div id="portfolio-details" class="portfolio-details">
        <div class="container">
  
          <div class="row">
  
            <div class="col-lg-12 portfolio-info">
              <br>
              <h2 style="color:#12d640">Development of a simulation of a lunar Lander using Deep Learning Q-Networks</h2>
              <p>The project aimed to achieve the successful landing of a lunar lander in a 2D environment by leveraging reinforcement learning and deep Q-networks.The complete code can be found in my github Page</p>
              <h2>Methodology</h2>
              <ol>
                <li>Package Import: The necessary packages and libraries were imported to support the implementation.</li>
                <li>Hyperparameters: The hyperparameters for the deep Q-learning algorithm were defined, including learning rate, discount factor, exploration rate, etc.</li>
                <li>
                  Lunar Lander Environment:
                  <ol>
                    <li>Action Space</li>
                    <li>Observation Space</li>
                    <li>Rewards</li>
                    <li>Episode Termination</li>
                  </ol>
                </li>
                <li>Environment Loading: The Lunar Lander environment was loaded for training and evaluation purposes.</li>
                <li>Interacting with the Gym Environment: The agent interacted with the Gym environment to observe its dynamics and understand the behavior of the environment.</li>
                <li>Update the Network Weights: The network weights were updated based on the loss calculated using the Bellman equation.</li>
                <li>Agent Training: The agent was trained using the deep Q-learning algorithm and experience replay, gradually improving its performance over multiple episodes.</li>
                <li>Trained Agent Evaluation: The trained agent was evaluated to observe its performance in the Lunar Lander environment.</li>
              </ol>
              <h3>Importing the Packages</h3>
                <li><code>numpy</code> is a package for scientific computing in Python.</li>
                <li><code>deque </code> will be our data structure for our memory buffer.</li>
                <li><code>namedtuple</code> will be used to store the experience tuples.</li>
                <li>The <code>gym toolkit</code> is a collection of environments that can be used to test reinforcement learning algorithms.</li>
                <li><code>PIL.Image</code> and <code>pyvirtualdisplay</code> are needed to render the Lunar Lander environment.</li>
                <li><code>tensorflow.keras</code> framework for building deep learning models.</li>
                <li><code>utils</code> is a module that contains helper functions</li><br>
              <h3>Hyperparameters</h3>
              <p>Hyperparameters refer to the configuration settings or parameters that define the behavior and performance of a machine learning model. Unlike model parameters, which are learned from the data during the training process, hyperparameters are set before training and remain fixed throughout the training process. They influence how the model learns and generalizes from the data.</p>
              <p>The Hyper parameters of the lunar landing simulator is as follows:
                <li><code>MEMORY_SIZE = 100_000</code>     # size of memory buffer</li>
                <li><code>GAMMA = 0.995</code>            # discount factor</li>
                <li><code>ALPHA = 1e-3</code>              # learning rate</li>
                <li><code>NUM_STEPS_FOR_UPDATE = 4</code>  # perform a learning update every C time steps</li>
              </p>
              <h3>Lunar Lander Environment</h3>
               <p>An environment represents a problem or task to be solved.The goal of the Lunar Lander environment is to land the lunar lander safely on the landing pad on the surface of the moon. The landing pad is designated by two flag poles and it is always at coordinates (0,0) but the lander is also allowed to land outside of the landing pad. The lander starts at the top center of the environment with a random initial force applied to its center of mass and has infinite fuel. The environment is considered solved if the learner earns 200 points.</p>
               <h4>Action Space</h4>
               <p>The agent has four discrete actions available:
                <ol>
                  <li>Do Nothing</li>
                  <li>Fire the right engine</li>
                  <li>Fire the main engine</li>
                  <li>Fire the left engine</li>
                </ol>
               </p><br>
               <p>Each action has a corresponding numerical value:
                <ol>
                  <li>Do nothing = 0</li>
                  <li>Fire the right engine = 1</li>
                  <li>Fire the main engine = 2</li>
                  <li>Fire the left engine = 3</li>
                </ol>
               </p>
               <h4>Observation Space</h4>
               <p>The agents Observation space consists of a state vector with 8 variables :</p>
               <div>
                <label for="coordinates">Coordinates (x, y)</label>
                <span id="coordinates"></span>
              </div>
              <div>
                <label for="linear-velocities">Linear Velocities (x', y')</label>
                <span id="linear-velocities"></span>
              </div>
              <div>
                <label for="angle">Angle (ùúÉ)</label>
                <span id="angle"></span>
              </div>
              <div>
                <label for="angular-velocity">Angular Velocity (ùúÉ')</label>
                <span id="angular-velocity"></span>
              </div>
              <div>
                <label for="left-leg-contact">Left Leg Contact: This is a boolean represented as l</label>
                <span id="left-leg-contact"></span>
              </div>
              <div>
                <label for="right-leg-contact">Right Leg Contact: This is a boolean represented as r</label>
                <span id="right-leg-contact"></span>
              </div>
              <h4>Rewards</h4>
              <div>
                <strong>Landing on the landing pad and coming to rest:</strong> about 100-140 points
              </div>
              <div>
                <strong>Moving away from the landing pad:</strong> loses reward
              </div>
              <div>
                <strong>Crashing:</strong> -100 points
              </div>
              <div>
                <strong>Coming to rest:</strong> +100 points
              </div>
              <div>
                <strong>Each leg with ground contact:</strong> +10 points
              </div>
              <div>
                <strong>Firing the main engine:</strong> -0.3 points each frame
              </div>
              <div>
                <strong>Firing the side engine:</strong> -0.03 points each frame
              </div><br>
              <h4>Episode Termination</h4>
              <div>
                <strong>Lunar lander crashes:</strong> if the body of the lunar lander comes in contact with the surface of the moon
              </div>
              <div>
                <strong>Absolute value of the lander's x-coordinate is greater than 1:</strong> it goes beyond the left or right border
              </div><br>
              <h3>Loading the Environment</h3>
              <p>The lunar lander environment is loaded by loading the <code>LunarLander-v2</code> environment from the <code>gym</code> library by using the <code>.make()</code></p>
              <p><code>env = gym.make('LunarLander-v2')</code> #code to load the gym environment.Once we load the environment we use the <code>reset() </code> method to reset the environment to the initial state. The lander starts at the top center of the environment and we can render the first frame of the environment by using the <code>.render()</code> method.</p>
              <p>In order to build our neural network later on we need to know the size of the state vector and the number of valid actions. We can get this information from our environment by using the <code>.observation_space.shape</code> and <code>action_space.n</code> methods, respectively.</p>
              <h3>Interacting with the Gym Environment</h3>
              <p>The Gym library implements the standard ‚Äúagent-environment loop‚Äù formalism:
                In the standard ‚Äúagent-environment loop‚Äù formalism, an agent interacts with the environment in discrete time steps  ùë°=0,1,2,.... At each time step  ùë°, the agent uses a policy  ùúã to select an action  ùê¥ùë° based on its observation of the environment's state  ùëÜùë°. The agent receives a numerical reward ùëÖùë° and on the next time step, moves to a new state ùëÜùë°+1.</p>
              <h4>Exploring the Environment's Dynamics</h4>
              <p>In Open AI's Gym environments, the <code>.step()</code>method is used to run a single time step of the environment's dynamics.The <code>.step()</code> method accepts an action and returns four values:
              <li><code>observation</code> (object): an environment-specific object representing your observation of the environment. In the Lunar Lander environment this corresponds to a numpy array containing the positions and velocities of the lander </li>
              <li><code>reward</code> (float): amount of reward returned as a result of taking the given action. In the Lunar Lander environment this corresponds to a float of type <code>numpy.float64</code></li>
              <li><code>done</code> (boolean): When done is True, it indicates the episode has terminated and it‚Äôs time to reset the environment.</li>
              <li><code>info</code> (dictionary): diagnostic information useful for debugging. We won't be using this variable in this notebook but it is shown here for completeness.</li><br>
              <p>To begin an episode the environment has to be in the initial state this is done by using the <code>.reset()</code> method</p>
              <P>Once the environment is reset, the agent can start taking actions in the environment by using the <code>.step()</code> method. Note that the agent can only take one action per time step. when the agent is trained a loop is used to allow the agent to take many consecutive actions during an episode.</P>
              <h3>Deep Learning Q-Network Learning</h3>
              <!DOCTYPE html>
              <html>
              <head>
              <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
              <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
              <style>
                .formula {
                  display: block;
                  margin-top: 10px;
                  margin-bottom: 10px;
                }
              </style>
              </head>
              <body>
              <p>In cases where both the state and action space are discrete, we can estimate the action-value function iteratively by using the Bellman equation:</p>
              <p class="formula">\[ Q_{i+1}(s,a) = R + \gamma \max_{a'} Q_i(s',a') \]</p>
              <p>This iterative method converges to the optimal action-value function \( Q^*(s,a) \) as \( i \rightarrow \infty \). This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of \( Q(s,a) \) until it converges to the optimal action-value function \( Q^*(s,a) \). However, in cases where the state space is continuous, it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate \( Q(s,a) \) until it converges to \( Q^*(s,a) \).</p>
              <p>In Deep Q-Learning,this problem is solved by using a neural network to estimate the action-value function \( Q(s,a) \approx Q^*(s,a) \).This neural network is called a Q-Network, and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.</p>
              <p>Unfortunately, using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Luckily, there are a couple of techniques that can be employed to avoid instabilities. These techniques consist of using a Target Network and Experience Replay.</p>
              </body>
              </html>
              <h4>Target Network</h4>
              <!DOCTYPE html>
<html>
<head>
  <!-- Add your CSS and other meta tags here -->
</head>
<body>
  <main id="main">
    <div class="content">
      <p>The Q-Network can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:</p>
      <p class="formula">\( y = R + \gamma \max_{a'} Q(s', a'; w) \)</p>
      <p>where \( w \) are the weights of the Q-Network. This means that the weights \( w \) are adjusted at each iteration to minimize the following error:</p>
      <div style="text-align: center; padding-bottom: 24px;"><img src="../assets/img/Screenshot (105).png" class="img-fluid" alt="" height="315" width="560"></div>
      <p>Notice that this forms a problem because the target \( y_{\text{target}} \) is changing on every iteration. To avoid oscillations and instabilities, a separate neural network is created for generating the target values.This separate neural network is called as the target Q-Network, denoted as \( \hat{Q} \), and it will have the same architecture as the original Q-Network.</p>
      <p>By using the target Q-Network, the error becomes:</p>
      <div style="text-align: center; padding-bottom: 24px;"><img src="../assets/img/Screenshot (106).png" class="img-fluid" alt="" height="315" width="560"></div>
      <p>where \( w^- \) and \( w \) are the weights of the target Q-Network and the Q-Network, respectively.</p>
      <p>In practice, the following algorithm is used : every \( C \) time step , the target Q-Network's are used to generate the target values (\( y \)) and the weights of the target Q-Network is updated by using a soft update:</p>
      <p class="formula">\( w^- \leftarrow \tau w + (1 - \tau) w^- \)</p>
      <p>By using the soft update, the target values are changed slowly, greatly improving the stability of the learning algorithm.</p>
    </div>
  </main>
</body>
</html>
<p> the Deep  ùëÑ
  -Network (DQN) is a neural network that approximates the action-value function  ùëÑ(ùë†,ùëé)‚âàùëÑ‚àó(ùë†,ùëé)
  . It does this by learning how to map states to  ùëÑ
   values.</p>
   <p>To solve the Lunar Lander environment, a DQN with the following architecture is employed:</p>
      <ul>
        <li>An Input layer that takes <em>state_size</em> as input.</li>
        <li>A Dense layer with 64 units and a ReLU activation function.</li>
        <li>A Dense layer with 64 units and a ReLU activation function.</li>
        <li>A Dense layer with <em>num_actions</em> units and a linear activation function. This is the output layer of the network.</li>
      </ul>
      <p>This architecture allows the DQN to learn and approximate the action-value function effectively. Lastly , Adam has to be set as the optimizer with the learning rate equal to ALPHA.This was defined in the Hyperparameters section</p>
      <h4>Experience Replay</h4>
      <p>When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this,a technique is employed known as Experience Replay to generate uncorrelated experiences for training the agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples  (ùëÜùë°,ùê¥ùë°,ùëÖùë°,ùëÜùë°+1)
        will be added to the memory buffer at each time step as the agent interacts with the environment.</p>
      <p>By using experience replay problematic correlations, oscillations and instabilities are avoided. In addition, experience replay also allows the agent to potentially use the same experience in multiple weight updates, which increases data efficiency.</p>
      <p>The experience replay is combined with the Deep Q-Learning algorithm to achieve effective learning</p>
      <h3> Updating the Network's Weights</h3>
      <p>The <code>agent_learn</code> function is used over here. The agent_learn function will update the weights of the  ùëÑ
        and target  ùëÑÃÇ 
        networks using a custom training loop. Because a custom training loop is used the gradients have to be retrived via a <code>tf.GradientTape</code> instance, and then call <code>optimizer.apply_gradients()</code> to update the weights of the  ùëÑ
       -Network. Note that the <code>@tf.function decorator</code> is also used to increase performance. Without this decorator the training of the network will take twice as long.</p>
      <h3>Training the Agent</h3>
      <p>
        <li>Line 1: Initialize the <code>memory_buffer</code> with a capacity of <code>N = MEMORY_SIZE</code>. Note that a <code>deque</code> is used as the data structure for the <code>memory_buffer</code>.</li>
        <li>Line 2: Initialize the <code>target_q_network</code> by setting its weights equal to those of the <code>q_network</code>.
        <li>Line 3: Start the outer loop with <code>M = num_episodes = 2000</code>. This number is reasonable as the agent is expected to solve the Lunar Lander environment in less than 2000 episodes using the defined parameters.</li>
        <li>Line 4: Reset the environment to the initial state and obtain the initial state.</li>
        <li>Line 5: Start the inner loop with <code>T = max_num_timesteps = 1000</code>. The episode terminates automatically if it hasn't terminated after 1000 time steps.</li>
        <li>Line 6: Observe the current state and choose an action using an Œµ-greedy policy. The agent starts with Œµ = epsilon = 1, resulting in an Œµ-greedy policy equivalent to the equiprobable random policy. As training progresses, decrease Œµ slowly towards a minimum value using a given Œµ-decay rate. The minimum Œµ value is set to 0.01 to maintain a small amount of exploration during training.</li>
        <li>Line 7: Take the chosen action in the environment and obtain the reward and the next_state.</li>
        <li>Line 8: Store the <code>experience (state, action, reward, next_state, done)</code> tuple in the <code>memory_buffer</code>, including the done variable to track episode termination for setting the y targets.</li>
        <li>Line 9: Check if the conditions are met to perform a learning update using the custom ,<code>utils.check_update_conditions</code> function. This function verifies if <code>C = NUM_STEPS_FOR_UPDATE</code> = 4 time steps have occurred and if the <code>memory_buffer</code> has enough experience tuples to fill a mini-batch.</li>
        <li>Lines 10 - 13: If the update variable is True, perform a learning update. This includes sampling a random mini-batch of experience tuples from the <code>memory_buffer</code>, setting the y targets, performing gradient descent, and updating the weights of the networks using the <code>agent_learn</code> function.</li>
        <li>Line 14: Set next_state as the new state at the end of each inner loop iteration. Additionally, check if the episode has reached a terminal state <code>(done = True)</code> and break out of the inner loop if so.</li>
        <li>Line 15: Update the value of Œµ at the end of each outer loop iteration and check if the environment has been solved. Consider the environment solved if the agent receives an average of 200 points in the last 100 episodes. If not solved, continue the outer loop and start a new episode.</li><br>
  
        Note: Additional variables are included to track the total number of points the agent received in each episode. This helps determine if the environment has been solved and provides insight into the agent's performance during training. The time module is used to measure the training duration.
        </p>

        <p>The below Plot shows how the Agent improved during its training</p>
        <div style="text-align: center; padding-bottom: 24px;"><img src="../assets/img/Agent Training.png" class="img-fluid" alt="" height="315" width="560"></div>

        <h2>Output</h2>
        <p>Now that the agent is trained , it can be seen in action. The <code>utils.create_video function</code> is used to create a video of the agent interacting with the environment using the trained  ùëÑ
          -Network. The <code>utils.create_video function</code> uses the <code>imageio library</code> to create the video.The video of the Lunar lander landing on the landing pad can be seen in the below video</p><br>
          <div style="text-align: center; padding-bottom: 24px;"><iframe width="560" height="315" src="https://www.youtube.com/embed/vv_w-kD1rVQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><br><br></div>
        <h2>Future Scope</h2>
        <p>Reinforcement learning in robotics holds immense potential for advancing autonomous systems. By leveraging the power of reinforcement learning algorithms, robots can learn and adapt to complex environments, enabling them to perform a wide range of tasks with increased efficiency and intelligence. The integration of reinforcement learning techniques in robotics opens doors to advancements in areas such as robotic manipulation, autonomous navigation, human-robot interaction, and multi-robot systems. With further research and development, reinforcement learning has the potential to revolutionize the field of robotics, paving the way for highly capable and adaptable robotic systems that can operate in diverse real-world scenarios.</p>
        <p>By delving into the world of reinforcement learning and mastering the concepts of Deep Q-Networks (DQN), I have unlocked a powerful tool that propels me towards becoming a proficient robotics engineer. Reinforcement learning, with its ability to train intelligent agents to make optimal decisions in dynamic environments, serves as a vital stepping stone in my journey. It has not only expanded my knowledge but also brought me closer to achieving my goals in the exciting realm of robotics.</p>
        <h2>References</h2>
        <li>Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529‚Äì533 (2015).</li>
        <li>Lillicrap, T. P., Hunt, J. J., Pritzel, A., et al. Continuous Control with Deep Reinforcement Learning. ICLR (2016).</li>
        <li>Mnih, V., Kavukcuoglu, K., Silver, D. et al. Playing Atari with Deep Reinforcement Learning. arXiv e-prints. arXiv:1312.5602 (2013).</li>

            </div>
  
          </div>
  
        </div>
      </div><!-- End Portfolio Details -->
    </main><!-- End #main -->
  
   <!-- Vendor JS Files -->
    <script async="" src="//www.google-analytics.com/analytics.js"></script><script src="assets/vendor/jquery/jquery.min.js"></script>
    <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="../assets/vendor/jquery.easing/jquery.easing.min.js"></script>
    <script src="../assets/vendor/php-email-form/validate.js"></script>
    <script src="../assets/vendor/waypoints/jquery.waypoints.min.js"></script>
    <script src="../assets/vendor/counterup/counterup.min.js"></script>
    <script src="../assets/vendor/owl.carousel/owl.carousel.min.js"></script>
    <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script src="../assets/vendor/venobox/venobox.min.js"></script>
  
    <!-- Template Main JS File -->
    <script src="../assets/js/main.js"></script>
  
    <script>if( window.self == window.top ) { (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-55234356-4', 'auto'); ga('send', 'pageview'); } </script>
  
  </body>
  </html>
  
  